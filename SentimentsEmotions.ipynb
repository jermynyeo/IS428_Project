{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: text2emotion in c:\\users\\stary_nie3ly3\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.5)\n",
      "Requirement already satisfied: emoji>=0.6.0 in c:\\users\\stary_nie3ly3\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from text2emotion) (1.7.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\stary_nie3ly3\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from text2emotion) (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\stary_nie3ly3\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->text2emotion) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\stary_nie3ly3\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->text2emotion) (4.63.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\stary_nie3ly3\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->text2emotion) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\stary_nie3ly3\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->text2emotion) (2022.3.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\stary_nie3ly3\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk->text2emotion) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install text2emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import glob \n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "import text2emotion as te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getJSONData(file):\n",
    "    df = pd.read_json(file, lines=True)\n",
    "    data = df[df.lang == \"en\"].loc[:, ['content']]\n",
    "    data[\"sentiments\"] = data.content.apply(lambda x: getSentimentEmotion(x))\n",
    "    data[['neg','neu', 'pos','comp', 'angry', 'fear', 'happy', 'sad', 'surprise']] = data['sentiments'].str.split(',',expand=True)\n",
    "    data.drop([\"sentiments\"], inplace=True, axis = 1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getSentimentEmotion(tweet):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    tweet_words = [t.lower() for t in tweet.split() if re.match(r\"(\\w+)\", t)]\n",
    "    text = \" \".join(tweet_words)\n",
    "    ss = sid.polarity_scores(text)\n",
    "    emo = te.get_emotion(text)\n",
    "    return f'{ss[\"neg\"]}, {ss[\"neu\"]}, {ss[\"pos\"]}, {ss[\"compound\"]}, {emo[\"Angry\"]}, {emo[\"Fear\"]}, {emo[\"Happy\"]}, {emo[\"Sad\"]}, {emo[\"Surprise\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/tweets\\13 reasons why.json\n",
      "Data/tweets\\all of us are dead.json\n",
      "Data/tweets\\bridgerton.json\n",
      "Data/tweets\\maid.json\n",
      "Data/tweets\\money heist.json\n",
      "Data/tweets\\red notice.json\n",
      "Data/tweets\\squid game.json\n",
      "Data/tweets\\stranger things.json\n",
      "Data/tweets\\the witcher.json\n",
      "Data/tweets\\you.json\n"
     ]
    }
   ],
   "source": [
    "FILE_DIR = \"Data/tweets/\"\n",
    "overalldf = pd.DataFrame()\n",
    "csv_files = glob.glob(f\"{FILE_DIR}*.json\")\n",
    "for file in csv_files: \n",
    "    print (file)\n",
    "    data = getJSONData(file)\n",
    "    data.to_csv(f\"{file.split('.')[0]}.csv\")\n",
    "    overalldf = overalldf.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "overalldf.to_csv(f\"{FILE_DIR}processed/final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd3e321934a89039be24996b5fc6782ee7b909f22b294d7652f2f30f3effe4cb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
